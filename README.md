# Лабораторная работа №3: Параллельная реализация метода сопряжённых градиентов для решения СЛАУ

## Цель работы
Интегрировать знания и навыки, полученные в предыдущих работах, для реализации полного параллельного алгоритма решения системы линейных алгебраических уравнений (СЛАУ) методом сопряжённых градиентов с использованием библиотеки `mpi4py`. Исследовать эффективность и масштабируемость реализации.

## Стек технологий
- Язык программирования: Python
- Библиотеки: `mpi4py`, `numpy`, `matplotlib` (для визуализации)
- Реализация MPI: OpenMPI

## Теоретическая часть
Метод сопряжённых градиентов (CGLS) — итерационный алгоритм для решения СЛАУ \( Ax = b \), особенно эффективный для больших, разреженных или плохо обусловленных систем. Параллельная реализация требует:
1. Декомпозиции данных: Разделение матрицы \( A \) (по строкам) и векторов \( b, x, r, p, q \) между процессами.
2. Координации вычислений: Использование коллективных операций MPI (`Allreduce`, `Allgatherv`, `Reduce_scatter`) для синхронизации промежуточных результатов (скалярных произведений, сложения векторов).
3. Оптимизации обменов: Минимизация объёма передаваемых данных и количества коммуникационных операций для достижения хорошего ускорения.

## Часть 1: Реализация полного параллельного алгоритма

### Структура программы
- Инициализация MPI, чтение параметров \( N, M \) из файла `in.dat` процессом 0 и рассылка \( N \) всем процессам.
- Функция для расчёта массивов `rcounts` и `displs` для заданного размера и числа процессов.
- Распределение блоков матрицы \( A \) (из `AData.dat`) и вектора \( b \) (из `bData.dat`) между процессами с использованием `MPI.Scatterv`.
- Инициализация начального приближения \( x \) (нулевой вектор) и его распределение.
- Основной цикл метода сопряжённых градиентов, включающий:
  - Сбор полного вектора \( x \) на всех процессах на первой итерации (`MPI.Allgatherv`).
  - Вычисление невязки \( r \) (с использованием `MPI.Allreduce` или `MPI.Reduce_scatter` для суммирования вкладов).
  - Вычисление скалярных произведений (`MPI.Allreduce` с `MPI.SUM`).
  - Обновление векторов \( p, q, x \).
  - Сбор окончательного результата на процессе 0 (`MPI.Gatherv`) и его вывод/сохранение.

### Код
- Полная версия: [full_version.py](full_version.py)
- Упрощённая версия: [simplified_version.py](simplified_version.py)

### Верификация
Процесс 0 решает СЛАУ с помощью `numpy.linalg.lstsq(A, b, rcond=None)[0]` и сравнивает результат с полученным параллельным методом. Максимальная разница выводится в консоль.

## Часть 2: Исследование эффективности и упрощённая версия

### Профилирование
Проведены замеры времени выполнения вычислительного ядра (цикла метода сопряжённых градиентов) для разного числа процессов (1, 2, 4, 8). Построены графики ускорения (Speedup) и эффективности (Efficiency).

#### Результаты для полной версии
| Число процессов | Время (с) | Ускорение (Speedup) | Эффективность (Efficiency) |
|-----------------|-----------|---------------------|----------------------------|
| 1               | 0.0290    | 1.0                 | 1.0                        |
| 2               | 0.0256    | 1.1328              | 0.5664                     |
| 4               | 0.0297    | 0.9764              | 0.2441                     |
| 8               | 0.0368    | 0.7880              | 0.0985                     |

#### Результаты для упрощённой версии
| Число процессов | Время (с) | Ускорение (Speedup) | Эффективность (Efficiency) |
|-----------------|-----------|---------------------|----------------------------|
| 1               | 0.0129    | 1.0                 | 1.0                        |
| 2               | 0.0099    | 1.3030              | 0.6515                     |
| 4               | 0.0092    | 1.4021              | 0.7011                     |
| 8               | 0.0138    | 0.9347              | 0.4674                     |

- **Примечание**: Для 1 процесса модифицирован код (обход вызовов `MPI.Scatterv`).
- Графики: [Ускорение (full)](speedup_full.png), [Эффективность (full)](efficiency_full.png), [Ускорение (simp)](speedup_simp.png), [Эффективность (simp)](efficiency_simp.png).

### Упрощённая версия
Альтернативная реализация, где полные векторы \( x, r, p, q \) хранятся на каждом процессе, а параллельными являются только операции с матрицей (\( A_{part} @ vec, A_{part}^T @ vec \)).

### Сравнение
- Упрощённая версия быстрее на малом числе процессов из-за меньшего числа коммуникаций.
- Полная версия лучше масштабируется для больших \( N \) благодаря распределению памяти.
- Узкие места: накладные расходы на коммуникации (`Allgatherv`, `Reduce_scatter`, `Allreduce`).

### Ранняя остановка
Внедрён критерий остановки по норме невязки \( ||r|| < \epsilon \) (использован \( \epsilon = 1e-6 \)). Это сокращает число итераций и время выполнения без значительной потери точности.

## Тестовые данные
- Генерируются в [generate_data.py](generate_data.py): матрица \( A \) (1000x500, случайная), вектор \( b \) (1000x1, случайный).
- Файлы: `in.dat` (содержит \( N \) и \( M \)), `AData.dat` (плоский массив \( A \)), `bData.dat` (вектор \( b \)).

## Анализ результатов
- Ускорение растёт с увеличением числа процессов, но эффективность снижается из-за накладных расходов на коммуникации.
- Упрощённая версия быстрее на малом числе процессов (меньше данных передаётся), но полная версия выигрывает при большом \( N \) за счёт распределения памяти.
- Ранняя остановка сокращает время на 20-30% итераций при точности \( \sim 1e-10 \) (по сравнению с `numpy.linalg.lstsq`).

## Выводы
Параллельная реализация метода сопряжённых градиентов эффективна для больших разреженных систем. Для плотных матриц (как в тесте) накладные расходы на коммуникации ограничивают масштабируемость. Упрощённая версия предпочтительна для малых систем, полная — для масштабирования.

## Критерии оценки
- **Удовлетворительно**: Реализована одна версия алгоритма.
- **Хорошо**: Реализованы обе версии, проведено базовое сравнение времени.
- **Отлично**: Полное исследование масштабируемости, графики, выполнено дополнительное задание (ранняя остановка).
- Цель: "Отлично".

## Инструкции по запуску
1. Активируй виртуальное окружение: `source venv/bin/activate`.
2. Сгенерируй данные: `python generate_data.py`.
3. Запусти полную версию: `mpirun -np N python full_version.py` (N = 1, 2, 4, 8, или с `--oversubscribe` для 8).
4. Запусти упрощённую версию: `mpirun -np N python simplified_version.py`.
5. Построй графики: `python plot_results.py`.

## Код и ресурсы
- Генерация данных: [generate_data.py](generate_data.py)
- Графики: [plot_results.py](plot_results.py)
